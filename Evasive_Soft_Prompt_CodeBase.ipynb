{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "orKqutenN2Fg",
        "0CagS6Oggo4B",
        "czS-mkKAw4NM"
      ],
      "gpuClass": "premium",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Resources:\n",
        "\n",
        "\n",
        "\n",
        "*  Using huggingface PEFT https://github.com/lxe/simple-llama-finetuner/blob/master/main.py\n",
        "*   PEFT Examples: https://github.com/huggingface/peft/tree/main/examples\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zqfLkrc1Zuv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "PbsP1L04OAvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lvwerra/trl.git\n",
        "!pip install trl/\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install loralib\n",
        "!pip install bitsandbytes\n",
        "!pip install openai\n",
        "!pip install datasets\n",
        "!pip install sentencepiece\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "3veklEjTFED_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune OpenAI-GPT2 Detector"
      ],
      "metadata": {
        "id": "czS-mkKAw4NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "class EncodedDataset(Dataset):\n",
        "\n",
        "  def __init__(self, input_sents: List[str],\n",
        "                input_labels: List[int],\n",
        "                tokenizer: PreTrainedTokenizer,\n",
        "                max_sequence_length: int = None):\n",
        "\n",
        "    self.input_sents = input_sents\n",
        "    self.input_labels = input_labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_sents)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    text = self.input_sents[index]\n",
        "    label = self.input_labels[index]\n",
        "\n",
        "    token = self.tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
        "\n",
        "    input_ids, mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
        "\n",
        "    return input_ids, mask_ids, label"
      ],
      "metadata": {
        "id": "sGDyM3ei0MgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AutoConfig\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Define the OneHotEncoder object\n",
        "encoder = OneHotEncoder(categories=[['0', '1']])\n",
        "\n",
        "detector_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base-openai-detector\")\n",
        "detectot_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base-openai-detector\")\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return detectot_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "def labelize_function(examples):\n",
        "# Convert the labels to integer values\n",
        "    labels = [int(label) for label in examples[\"label\"]]\n",
        "    encoder.fit(np.array(labels).reshape(-1, 1))\n",
        "\n",
        "    # One-hot encode the labels using the encoder\n",
        "    encoded_labels = encoder.transform(np.array(labels).reshape(-1, 1)).toarray()\n",
        "\n",
        "    return {\"input_ids\": examples[\"input_ids\"], \"attention_mask\": examples[\"attention_mask\"], \"labels\": encoded_labels}\n",
        "\n",
        "\n",
        "def detector_ft(train_dataset, path, batch_size):\n",
        "\n",
        "    tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets = tokenized_datasets.map(labelize_function, batched=True)\n",
        "    print(tokenized_datasets)\n",
        "\n",
        "\n",
        "    training_args = TrainingArguments(output_dir=str(path),\n",
        "                                    overwrite_output_dir=True,\n",
        "                                    evaluation_strategy=\"epoch\",\n",
        "                                    learning_rate=2e-5,\n",
        "                                    weight_decay=0.01,\n",
        "                                    num_train_epochs=15,\n",
        "                                    per_device_train_batch_size=batch_size,\n",
        "                                    per_device_eval_batch_size=batch_size)\n",
        "\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "      logits, labels = eval_pred\n",
        "      predictions = np.argmax(logits, axis=-1)\n",
        "      labels = np.argmax(labels, axis=-1)\n",
        "      return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=detector_model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"test\"],\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()"
      ],
      "metadata": {
        "id": "9v-Kl1xb0pvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "batch_size = 8\n",
        "data_path = \"\"\n",
        "\n",
        "save_path = \"\"\n",
        "\n",
        "train_path = data_path + \"train.csv\"\n",
        "test_path = data_path + \"test.csv\"\n",
        "\n",
        "ft_data_files = {\"train\": train_path, \"test\":test_path}\n",
        "\n",
        "ft_dataset = load_dataset(\"csv\", data_files=ft_data_files, sep=\",\")\n",
        "\n",
        "detector_ft(ft_dataset,save_path, batch_size)"
      ],
      "metadata": {
        "id": "G5DRb9eF19WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evasive Soft Prompt Learning : OpenAI-FT"
      ],
      "metadata": {
        "id": "dO9f_avZWQkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
        "from peft import get_peft_config, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, pipeline, LlamaTokenizer, LlamaForCausalLM, AutoModelForSequenceClassification\n",
        "\n",
        "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import datasets\n",
        "import transformers\n",
        "import re\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "# import tqdm\n",
        "import random\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import json\n",
        "import functools\n",
        "# import custom_datasets\n",
        "from multiprocessing.pool import ThreadPool\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import precision_score, \\\n",
        "    recall_score, confusion_matrix, classification_report, \\\n",
        "    accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "25uRBwRl1Tpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Detector: Zeroshot (DetectGPT)\n",
        "\n",
        "\n",
        "\n",
        "*   Original Source Code: https://github.com/eric-mitchell/detect-gpt\n",
        "\n"
      ],
      "metadata": {
        "id": "qKw6LrE8yZ-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_base_model():\n",
        "    print('MOVING BASE MODEL TO GPU...', end='', flush=True)\n",
        "    start = time.time()\n",
        "    try:\n",
        "        mask_model.cpu()\n",
        "    except NameError:\n",
        "        pass\n",
        "    if script_args.openai_model is None:\n",
        "        base_model.to(DEVICE)\n",
        "    print(f'DONE ({time.time() - start:.2f}s)')\n",
        "\n",
        "\n",
        "def load_mask_model():\n",
        "    print('MOVING MASK MODEL TO GPU...', end='', flush=True)\n",
        "    start = time.time()\n",
        "\n",
        "    if script_args.openai_model is None:\n",
        "        base_model.cpu()\n",
        "\n",
        "    if not script_args.random_fills:\n",
        "        mask_model.to(DEVICE)\n",
        "    print(f'DONE ({time.time() - start:.2f}s)')\n",
        "\n",
        "\n",
        "def tokenize_and_mask(text, span_length, pct, ceil_pct=False):\n",
        "    tokens = text.split(' ')\n",
        "    mask_string = '<<<mask>>>'\n",
        "\n",
        "    n_spans = pct * len(tokens) / (span_length + script_args.buffer_size * 2)\n",
        "    if ceil_pct:\n",
        "        n_spans = np.ceil(n_spans)\n",
        "    n_spans = int(n_spans)\n",
        "\n",
        "    n_masks = 0\n",
        "    while n_masks < n_spans:\n",
        "        start = np.random.randint(0, len(tokens) - span_length)\n",
        "        end = start + span_length\n",
        "        search_start = max(0, start - script_args.buffer_size)\n",
        "        search_end = min(len(tokens), end + script_args.buffer_size)\n",
        "        if mask_string not in tokens[search_start:search_end]:\n",
        "            tokens[start:end] = [mask_string]\n",
        "            n_masks += 1\n",
        "\n",
        "    # replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n",
        "    num_filled = 0\n",
        "    for idx, token in enumerate(tokens):\n",
        "        if token == mask_string:\n",
        "            tokens[idx] = f'<extra_id_{num_filled}>'\n",
        "            num_filled += 1\n",
        "    assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "def count_masks(texts):\n",
        "    return [len([x for x in text.split() if x.startswith(\"<extra_id_\")]) for text in texts]\n",
        "\n",
        "\n",
        "# replace each masked span with a sample from T5 mask_model\n",
        "def replace_masks(texts):\n",
        "    n_expected = count_masks(texts)\n",
        "    stop_id = mask_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
        "    tokens = mask_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "    outputs = mask_model.generate(**tokens, max_length=150, do_sample=True, top_p=script_args.mask_top_p, num_return_sequences=1, eos_token_id=stop_id)\n",
        "    return mask_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
        "\n",
        "\n",
        "def extract_fills(texts):\n",
        "    # remove <pad> from beginning of each text\n",
        "    texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in texts]\n",
        "\n",
        "    # return the text in between each matched mask token\n",
        "    extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n",
        "\n",
        "    # remove whitespace around each fill\n",
        "    extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n",
        "\n",
        "    return extracted_fills\n",
        "\n",
        "\n",
        "def apply_extracted_fills(masked_texts, extracted_fills):\n",
        "    # split masked text into tokens, only splitting on spaces (not newlines)\n",
        "    tokens = [x.split(' ') for x in masked_texts]\n",
        "\n",
        "    n_expected = count_masks(masked_texts)\n",
        "\n",
        "    # replace each mask token with the corresponding fill\n",
        "    for idx, (text, fills, n) in enumerate(zip(tokens, extracted_fills, n_expected)):\n",
        "        if len(fills) < n:\n",
        "            tokens[idx] = []\n",
        "        else:\n",
        "            for fill_idx in range(n):\n",
        "                text[text.index(f\"<extra_id_{fill_idx}>\")] = fills[fill_idx]\n",
        "\n",
        "    # join tokens back into text\n",
        "    texts = [\" \".join(x) for x in tokens]\n",
        "    return texts\n",
        "\n",
        "\n",
        "def perturb_texts_(texts, span_length, pct, ceil_pct=False):\n",
        "    if not script_args.random_fills:\n",
        "        masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for x in texts]\n",
        "        raw_fills = replace_masks(masked_texts)\n",
        "        extracted_fills = extract_fills(raw_fills)\n",
        "        perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
        "\n",
        "        # Handle the fact that sometimes the model doesn't generate the right number of fills and we have to try again\n",
        "        attempts = 1\n",
        "        while '' in perturbed_texts:\n",
        "            idxs = [idx for idx, x in enumerate(perturbed_texts) if x == '']\n",
        "            print(f'WARNING: {len(idxs)} texts have no fills. Trying again [attempt {attempts}].')\n",
        "            masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for idx, x in enumerate(texts) if idx in idxs]\n",
        "            raw_fills = replace_masks(masked_texts)\n",
        "            extracted_fills = extract_fills(raw_fills)\n",
        "            new_perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
        "            for idx, x in zip(idxs, new_perturbed_texts):\n",
        "                perturbed_texts[idx] = x\n",
        "            attempts += 1\n",
        "    else:\n",
        "        if script_args.random_fills_tokens:\n",
        "            # tokenize base_tokenizer\n",
        "            tokens = base_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "            valid_tokens = tokens.input_ids != base_tokenizer.pad_token_id\n",
        "            replace_pct = script_args.pct_words_masked * (script_args.span_length / (script_args.span_length + 2 * script_args.buffer_size))\n",
        "\n",
        "            # replace replace_pct of input_ids with random tokens\n",
        "            random_mask = torch.rand(tokens.input_ids.shape, device=DEVICE) < replace_pct\n",
        "            random_mask &= valid_tokens\n",
        "            random_tokens = torch.randint(0, base_tokenizer.vocab_size, (random_mask.sum(),), device=DEVICE)\n",
        "            # while any of the random tokens are special tokens, replace them with random non-special tokens\n",
        "            while any(base_tokenizer.decode(x) in base_tokenizer.all_special_tokens for x in random_tokens):\n",
        "                random_tokens = torch.randint(0, base_tokenizer.vocab_size, (random_mask.sum(),), device=DEVICE)\n",
        "            tokens.input_ids[random_mask] = random_tokens\n",
        "            perturbed_texts = base_tokenizer.batch_decode(tokens.input_ids, skip_special_tokens=True)\n",
        "        else:\n",
        "            masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for x in texts]\n",
        "            perturbed_texts = masked_texts\n",
        "            # replace each <extra_id_*> with script_args.span_length random words from FILL_DICTIONARY\n",
        "            for idx, text in enumerate(perturbed_texts):\n",
        "                filled_text = text\n",
        "                for fill_idx in range(count_masks([text])[0]):\n",
        "                    fill = random.sample(FILL_DICTIONARY, span_length)\n",
        "                    filled_text = filled_text.replace(f\"<extra_id_{fill_idx}>\", \" \".join(fill))\n",
        "                assert count_masks([filled_text])[0] == 0, \"Failed to replace all masks\"\n",
        "                perturbed_texts[idx] = filled_text\n",
        "\n",
        "    return perturbed_texts\n",
        "\n",
        "\n",
        "def perturb_texts(texts, span_length, pct, ceil_pct=False):\n",
        "    chunk_size = script_args.chunk_size\n",
        "    if '11b' in mask_filling_model_name:\n",
        "        chunk_size //= 2\n",
        "\n",
        "    outputs = []\n",
        "    for i in tqdm(range(0, len(texts), chunk_size), desc=\"Applying perturbations\"):\n",
        "        outputs.extend(perturb_texts_(texts[i:i + chunk_size], span_length, pct, ceil_pct=ceil_pct))\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def drop_last_word(text):\n",
        "    return ' '.join(text.split(' ')[:-1])\n",
        "\n",
        "\n",
        "def _openai_sample(p, args):\n",
        "    if script_args.dataset != 'pubmed':  # keep Answer: prefix for pubmed\n",
        "        p = drop_last_word(p)\n",
        "\n",
        "    # sample from the openai model\n",
        "    kwargs = { \"engine\": script_args.openai_model, \"max_tokens\": 200 }\n",
        "    if script_args.do_top_p:\n",
        "        kwargs['top_p'] = script_args.top_p\n",
        "\n",
        "    r = openai.Completion.create(prompt=f\"{p}\", **kwargs)\n",
        "    return p + r['choices'][0].text\n",
        "\n",
        "\n",
        "def get_likelihood(logits, labels):\n",
        "    assert logits.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "\n",
        "    logits = logits.view(-1, logits.shape[-1])[:-1]\n",
        "    labels = labels.view(-1)[1:]\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "    log_likelihood = log_probs.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
        "    return log_likelihood.mean()\n",
        "\n",
        "\n",
        "# Get the log likelihood of each text under the base_model\n",
        "def get_ll(text):\n",
        "    if script_args.openai_model:\n",
        "        kwargs = { \"engine\": script_args.openai_model, \"temperature\": 0, \"max_tokens\": 0, \"echo\": True, \"logprobs\": 0}\n",
        "        r = openai.Completion.create(prompt=f\"<|endoftext|>{text}\", **kwargs)\n",
        "        result = r['choices'][0]\n",
        "        tokens, logprobs = result[\"logprobs\"][\"tokens\"][1:], result[\"logprobs\"][\"token_logprobs\"][1:]\n",
        "\n",
        "        assert len(tokens) == len(logprobs), f\"Expected {len(tokens)} logprobs, got {len(logprobs)}\"\n",
        "\n",
        "        return np.mean(logprobs)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            tokenized = base_tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "            labels = tokenized.input_ids\n",
        "            return -base_model(input_ids=tokenized.input_ids,attention_mask=tokenized.attention_mask, labels=labels).loss.item()\n",
        "\n",
        "\n",
        "def get_lls(texts):\n",
        "    if not script_args.openai_model:\n",
        "        return [get_ll(text) for text in texts]\n",
        "    else:\n",
        "        global API_TOKEN_COUNTER\n",
        "\n",
        "        # use GPT2_TOKENIZER to get total number of tokens\n",
        "        total_tokens = sum(len(GPT2_TOKENIZER.encode(text)) for text in texts)\n",
        "        API_TOKEN_COUNTER += total_tokens * 2  # multiply by two because OpenAI double-counts echo_prompt tokens\n",
        "\n",
        "        pool = ThreadPool(script_args.batch_size)\n",
        "        return pool.map(get_ll, texts)\n",
        "\n",
        "\n",
        "def get_roc_metrics(preds):\n",
        "    fpr, tpr, _ = roc_curve([1] * len(preds), preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return fpr.tolist(), tpr.tolist(), float(roc_auc)\n",
        "\n",
        "\n",
        "def get_precision_recall_metrics(preds):\n",
        "    precision, recall, _ = precision_recall_curve( [1] * len(preds), preds)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    return precision.tolist(), recall.tolist(), float(pr_auc)\n",
        "\n",
        "\n",
        "def run_detectgpt_perturb(batch_text, span_length=10, n_perturbations=1, n_samples=500):\n",
        "    # load_mask_model()\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    perturb_fn = functools.partial(perturb_texts, span_length=span_length, pct=script_args.pct_words_masked)\n",
        "\n",
        "    p_batch_text = perturb_fn([x for x in batch_text for _ in range(n_perturbations)])\n",
        "\n",
        "    for _ in range(n_perturbation_rounds - 1):\n",
        "        try:\n",
        "            p_batch_text = perturb_fn(p_batch_text)\n",
        "        except AssertionError:\n",
        "            break\n",
        "\n",
        "    assert len(p_batch_text) == len(batch_text) * n_perturbations, f\"Expected {len(batch_text) * n_perturbations} perturbed samples, got {len(p_batch_text)}\"\n",
        "\n",
        "    for idx in range(len(batch_text)):\n",
        "        results.append({\n",
        "            \"original\": batch_text[idx],\n",
        "            \"perturbed_original\": p_batch_text[idx * n_perturbations: (idx + 1) * n_perturbations],\n",
        "        })\n",
        "\n",
        "    # load_base_model()\n",
        "\n",
        "    for res in tqdm(results, desc=\"Computing log likelihoods\"):\n",
        "        p_original_ll = get_lls(res[\"perturbed_original\"])\n",
        "        res[\"original_ll\"] = get_ll(res[\"original\"])\n",
        "\n",
        "        res[\"all_perturbed_original_ll\"] = p_original_ll\n",
        "\n",
        "        res[\"perturbed_original_ll\"] = np.mean(p_original_ll)\n",
        "\n",
        "        res[\"perturbed_original_ll_std\"] = np.std(p_original_ll) if len(p_original_ll) > 1 else 1\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def get_detectgpt_scores(results, criterion, span_length=10, n_perturbations=1, n_samples=500):\n",
        "    # compute diffs with perturbed\n",
        "    predictions = []\n",
        "    for res in results:\n",
        "        if criterion == 'd':\n",
        "            predictions.append(res['original_ll'] - res['perturbed_original_ll'])\n",
        "\n",
        "        elif criterion == 'z':\n",
        "            if res['perturbed_original_ll_std'] == 0:\n",
        "                res['perturbed_original_ll_std'] = 1\n",
        "                print(\"WARNING: std of perturbed original is 0, setting to 1\")\n",
        "                print(f\"Number of unique perturbed original texts: {len(set(res['perturbed_original']))}\")\n",
        "                print(f\"Original text: {res['original']}\")\n",
        "\n",
        "            predictions.append((res['original_ll'] - res['perturbed_original_ll']) / res['perturbed_original_ll_std'])\n",
        "\n",
        "    fpr, tpr, roc_auc = get_roc_metrics(predictions)\n",
        "    p, r, pr_auc = get_precision_recall_metrics(predictions)\n",
        "\n",
        "    name = f'perturbation_{n_perturbations}_{criterion}'\n",
        "    print(f\"{name} ROC AUC: {roc_auc}, PR AUC: {pr_auc}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def load_base_model_and_tokenizer(name):\n",
        "\n",
        "    if 'llama' in name:\n",
        "      base_model = transformers.AutoModelForCausalLM.from_pretrained(name, **base_model_kwargs, cache_dir=cache_dir)\n",
        "      base_tokenizer = transformers.AutoTokenizer.from_pretrained(name, cache_dir=script_args.cache_dir)\n",
        "      base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
        "\n",
        "    else:\n",
        "      base_tokenizer = transformers.AutoTokenizer.from_pretrained(name, cache_dir=script_args.cache_dir)\n",
        "      base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
        "\n",
        "    return base_model, base_tokenizer"
      ],
      "metadata": {
        "id": "FzkeMeV1ypYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  params = {\"model_name\": \"\",\n",
        "            \"log_with\" : None,\n",
        "            \"learning_rate\" : 1.41e-5,\n",
        "            \"mini_batch_size\" : 4,\n",
        "            \"batch_size\" : 4,\n",
        "            \"gradient_accumulation_steps\" : 1,\n",
        "            \"pct_words_masked\": 0.3,\n",
        "            \"span_length\": 2,\n",
        "            \"n_samples\":200,\n",
        "            \"n_perturbations\":5,\n",
        "            \"n_perturbation_rounds\":1,\n",
        "            \"base_model_name\": \"\",\n",
        "            \"mask_filling_model_name\": \"t5-base\",\n",
        "            \"random_fills\": True,\n",
        "            \"random_fills_tokens\":True,\n",
        "            \"buffer_size\":1,\n",
        "            \"chunk_size\":20,\n",
        "            \"mask_top_p\":1.0,\n",
        "            \"pre_perturb_pct\":0.0,\n",
        "            \"pre_perturb_span_length\":5,\n",
        "            \"n_similarity_samples\":20,\n",
        "            \"openai_model\":None,\n",
        "            \"cache_dir\":\"/content/sample_data\",\n",
        "            \"int8\": True,\n",
        "            \"half\":False\n",
        "            }\n",
        "\n",
        "  script_args = SimpleNamespace(**params)\n",
        "\n",
        "  mask_filling_model_name = script_args.mask_filling_model_name\n",
        "  n_samples = script_args.n_samples\n",
        "  batch_size = script_args.batch_size\n",
        "  # n_perturbation_list = [int(x) for x in script_args.n_perturbation_list.split(\",\")]\n",
        "  n_perturbation_rounds = script_args.n_perturbation_rounds\n",
        "  n_similarity_samples = script_args.n_similarity_samples\n",
        "\n",
        "  if script_args.int8:\n",
        "    int8_kwargs = dict(load_in_8bit=True, device_map='auto')\n",
        "\n",
        "  base_model = AutoModelForCausalLM.from_pretrained(script_args.base_model_name, **int8_kwargs, cache_dir=script_args.cache_dir)\n",
        "  base_tokenizer = AutoTokenizer.from_pretrained(script_args.base_model_name, cache_dir=script_args.cache_dir)\n",
        "  base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
        "\n",
        "  # mask filling t5 model\n",
        "  if not script_args.random_fills:\n",
        "      int8_kwargs = {}\n",
        "      half_kwargs = {}\n",
        "      if script_args.int8:\n",
        "          int8_kwargs = dict(load_in_8bit=True, device_map='auto', torch_dtype=torch.bfloat16)\n",
        "      elif script_args.half:\n",
        "          half_kwargs = dict(torch_dtype=torch.bfloat16)\n",
        "      print(f'Loading mask filling model {mask_filling_model_name}...')\n",
        "      mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(mask_filling_model_name, **int8_kwargs, **half_kwargs, cache_dir=script_args.cache_dir)\n",
        "      try:\n",
        "          n_positions = mask_model.config.n_positions\n",
        "      except AttributeError:\n",
        "          n_positions = 512\n",
        "  else:\n",
        "      n_positions = 512\n",
        "\n",
        "  # preproc_tokenizer = transformers.AutoTokenizer.from_pretrained('t5-small', model_max_length=512, cache_dir=script_args.cache_dir)\n",
        "  mask_tokenizer = transformers.AutoTokenizer.from_pretrained(mask_filling_model_name, model_max_length=n_positions, cache_dir=script_args.cache_dir)\n",
        "  preproc_tokenizer = mask_tokenizer\n",
        "\n",
        "  config = PPOConfig(\n",
        "      model_name=script_args.base_model_name,\n",
        "      learning_rate=script_args.learning_rate,\n",
        "      log_with=script_args.log_with,\n",
        "      mini_batch_size=script_args.mini_batch_size,\n",
        "      batch_size=script_args.batch_size,\n",
        "      gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
        "  )\n",
        "\n",
        "\n",
        "  def build_dataset(config, tokenizer, dataset_path=\"\", input_min_text_length=10, input_max_text_length=32):\n",
        "\n",
        "      ds = load_dataset(\"csv\", data_files=dataset_path)\n",
        "\n",
        "      def tokenize(sample):\n",
        "          # sample[\"input_ids\"] = tokenizer.encode(sample[\"title\"])[: input_size()]\n",
        "          sample[\"input_ids\"] = tokenizer.encode(sample[\"title\"])\n",
        "          sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "          return sample\n",
        "\n",
        "      ds = ds.map(tokenize, batched=False)\n",
        "      ds.set_format(type=\"torch\")\n",
        "\n",
        "      return ds\n",
        "\n",
        "  device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "  DEVICE = device\n",
        "\n",
        "  dataset = build_dataset(config, tokenizer=base_tokenizer)\n",
        "\n",
        "\n",
        "  def collator(data):\n",
        "      return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "  set_seed(config.seed)\n",
        "\n",
        "  \"\"\"### Apply PEFT\n",
        "  \"\"\"\n",
        "\n",
        "  def print_trainable_parameters(model):\n",
        "      \"\"\"\n",
        "      Prints the number of trainable parameters in the model.\n",
        "      \"\"\"\n",
        "      trainable_params = 0\n",
        "      all_param = 0\n",
        "      for _, param in model.named_parameters():\n",
        "          all_param += param.numel()\n",
        "          if param.requires_grad:\n",
        "              trainable_params += param.numel()\n",
        "      print(\n",
        "          f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "      )\n",
        "\n",
        "# Define the Soft Prompt Configurations Here: For Prompt Tuning Change it back to the commented section\n",
        "  lora_r = 8\n",
        "  lora_alpha= 16\n",
        "  lora_dropout = 0.05\n",
        "  lora_target_modules = [\"q_proj\",\"v_proj\",]\n",
        "\n",
        "  lora_config = LoraConfig(\n",
        "      r=lora_r,\n",
        "      lora_alpha=lora_alpha,\n",
        "      target_modules=lora_target_modules,\n",
        "      lora_dropout=lora_dropout,\n",
        "      bias=\"none\",\n",
        "      task_type=\"CAUSAL_LM\",\n",
        "  )\n",
        "\n",
        "  base_model = prepare_model_for_int8_training(base_model, output_embedding_layer_name=\"embed_out\")\n",
        "  base_model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "  model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model)\n",
        "\n",
        "  model.gradient_checkpointing_disable = model.pretrained_model.gradient_checkpointing_disable\n",
        "  model.gradient_checkpointing_enable = model.pretrained_model.gradient_checkpointing_enable\n",
        "\n",
        "  print_trainable_parameters(model)\n",
        "\n",
        "  optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n",
        "\n",
        "  print(\"Initialize PPO framework\")\n",
        "  ppo_trainer = PPOTrainer(\n",
        "      config, model, ref_model=None, tokenizer=base_tokenizer, dataset=dataset['train'], data_collator=collator, optimizer=optimizer\n",
        "  )\n",
        "\n",
        "\n",
        "  device = ppo_trainer.accelerator.device\n",
        "  if ppo_trainer.accelerator.num_processes == 1:\n",
        "      device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
        "\n",
        "  generation_kwargs = {\n",
        "      \"min_length\": -1,\n",
        "      \"top_k\": 0.0,\n",
        "      \"top_p\": 1.0,\n",
        "      \"do_sample\": True,\n",
        "      \"pad_token_id\": base_tokenizer.eos_token_id,\n",
        "      \"eos_token_id\": -1,\n",
        "  }\n",
        "\n",
        "  break_epoch = 100\n",
        "  gen_len = 32\n",
        "  print(\"Training starting........\")\n",
        "  for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "\n",
        "      if epoch== break_epoch:\n",
        "        break\n",
        "\n",
        "      query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "      print(\"Generation Started..\")\n",
        "\n",
        "      model.gradient_checkpointing_disable()\n",
        "      model.pretrained_model.config.use_cache = True\n",
        "\n",
        "      print(\"Generation Started..\")\n",
        "      # Get response from LLM - i.e. generations\n",
        "      response_tensors = []\n",
        "      for query in query_tensors:\n",
        "          # gen_len = output_length_sampler()\n",
        "          generation_kwargs[\"max_new_tokens\"] = gen_len\n",
        "          response = ppo_trainer.generate(query, **generation_kwargs)\n",
        "          response_tensors.append(response.squeeze()[-gen_len:])\n",
        "\n",
        "      print(\"Response generation completed...\")\n",
        "      batch[\"response\"] = [base_tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "\n",
        "      # Compute detector score - I added title + generation\n",
        "      texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "\n",
        "      results = run_detectgpt_perturb(texts, span_length=script_args.span_length, n_perturbations=script_args.n_perturbations)\n",
        "\n",
        "      pipe_outputs = get_detectgpt_scores(results, criterion='d', span_length=script_args.span_length, n_perturbations=script_args.n_perturbations)\n",
        "\n",
        "      print(\"Detector outputs: \", pipe_outputs)\n",
        "\n",
        "      rewards = [torch.tensor(output) for output in pipe_outputs]\n",
        "\n",
        "      # Run PPO step\n",
        "      model.gradient_checkpointing_enable()\n",
        "      model.pretrained_model.config.use_cache = False\n",
        "\n",
        "      print(\"PPO started...\")\n",
        "      # print(query_tensors)\n",
        "      # print(response_tensors)\n",
        "      # print(rewards)\n",
        "\n",
        "      stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "      ppo_trainer.log_stats(stats, batch, rewards)\n",
        "\n",
        "  peft_model_id = f\"\"\n",
        "  save_dir = \"\"\n",
        "  model.save_pretrained(save_dir+peft_model_id)"
      ],
      "metadata": {
        "id": "f7iFFi9WJtk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Detector: Supervised (OpenAI-FT)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jq6m5QubmAFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "class EncodedDataset(Dataset):\n",
        "\n",
        "  def __init__(self, input_sents: List[str],\n",
        "                input_labels: List[int],\n",
        "                tokenizer: PreTrainedTokenizer,\n",
        "                max_sequence_length: int = None,\n",
        "                max_targets: int = 5):\n",
        "\n",
        "    self.input_sents = input_sents\n",
        "    self.input_labels = input_labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    self.max_targets = max_targets\n",
        "    # self.min_sequence_length = min_sequence_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_sents)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    text = self.input_sents[index]\n",
        "    label = self.input_labels[index]\n",
        "\n",
        "    token = self.tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
        "\n",
        "    input_ids, mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
        "\n",
        "    return input_ids, mask_ids, label"
      ],
      "metadata": {
        "id": "_lJy9HguuKLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "def get_roc_metrics(y_true, preds):\n",
        "    fpr, tpr, _ = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return float(roc_auc)\n",
        "\n",
        "def evaluate_openaidetect(model, test_data, tokenizer, test_labels, max_sequence_length, test_batch_size, device, return_logits=False):\n",
        "    model = model.to(device)\n",
        "\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    test = EncodedDataset(input_sents=test_data,\n",
        "                    input_labels=test_labels,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_sequence_length=max_sequence_length)\n",
        "\n",
        "\n",
        "    test_dataloader = DataLoader(test, batch_size=test_batch_size)\n",
        "\n",
        "\n",
        "    total_acc_test = 0\n",
        "    total_loss_test = 0\n",
        "    predictions = []\n",
        "    y_true = []\n",
        "    output_logits = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for test_input, test_mask, test_label in test_dataloader:\n",
        "        test_input = test_input.to(device)\n",
        "        test_mask = test_mask.to(device)\n",
        "\n",
        "        # val_modifers = val_modifers.to(device)\n",
        "        test_label = test_label.to(device)\n",
        "\n",
        "        output = model(input_ids=test_input,\n",
        "                      attention_mask=test_mask)\n",
        "\n",
        "        logits = softmax(output.logits)\n",
        "\n",
        "        if return_logits == True:\n",
        "          output_logits.append(logits.detach().cpu())\n",
        "\n",
        "        acc = (logits.argmax(dim=1) == test_label).sum().item()\n",
        "\n",
        "        predictions.extend(logits.argmax(dim=1).detach().cpu().numpy())\n",
        "\n",
        "        y_true.extend(test_label.detach().cpu().numpy())\n",
        "\n",
        "      model.cpu()\n",
        "      if return_logits == True:\n",
        "          return predictions,y_true, output_logits\n",
        "\n",
        "      return predictions,y_true"
      ],
      "metadata": {
        "id": "NAV6ScaouQXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "params = {\"model_name\": \"\", #PLM Name\n",
        "          \"detector_ckpt\": \"\",\n",
        "          \"peft_model_id\":f\"\",\n",
        "          \"log_with\" : None,\n",
        "          \"learning_rate\" : 1.41e-5,\n",
        "          \"mini_batch_size\" : 4,\n",
        "          \"batch_size\" : 8,\n",
        "          \"gradient_accumulation_steps\" : 1,\n",
        "          }\n",
        "\n",
        "\n",
        "script_args = SimpleNamespace(**params)\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=script_args.model_name,\n",
        "    learning_rate=script_args.learning_rate,\n",
        "    log_with=script_args.log_with,\n",
        "    mini_batch_size=script_args.mini_batch_size,\n",
        "    batch_size=script_args.batch_size,\n",
        "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
        ")\n",
        "\n",
        "\n",
        "def build_dataset(config, tokenizer, dataset_path=\"\", input_min_text_length=10, input_max_text_length=32):\n",
        "\n",
        "    ds = load_dataset(\"csv\", data_files=dataset_path)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        # sample[\"input_ids\"] = tokenizer.encode(sample[\"title\"])[: input_size()]\n",
        "        sample[\"input_ids\"] = tokenizer.encode(sample[\"title\"])\n",
        "        sample[\"query\"] = sample[\"title\"]\n",
        "        return sample\n",
        "\n",
        "    ds = ds.map(tokenize, batched=False)\n",
        "    ds.set_format(type=\"torch\")\n",
        "\n",
        "    return ds\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pretrained_model = AutoModelForCausalLM.from_pretrained(config.model_name, load_in_8bit=True, device_map=\"auto\")\n",
        "\n",
        "if \"llama\" in script_args.model_name:\n",
        "  tokenizer = LlamaTokenizer.from_pretrained(config.model_name)\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "dataset = build_dataset(config, tokenizer=tokenizer)\n",
        "\n",
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "set_seed(config.seed)\n",
        "\n",
        "\n",
        "\"\"\"### Apply PEFT\n",
        "\"\"\"\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "# Define\n",
        "lora_r = 8\n",
        "lora_alpha= 16\n",
        "lora_dropout = 0.05\n",
        "\n",
        "lora_target_modules = None\n",
        "if \"gpt-neox\" in script_args.model_name:\n",
        "    lora_target_modules = [\"query_key_value\", \"xxx\"]  # workaround to use 8bit training on this model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=lora_target_modules,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "if \"gpt-neox\" in script_args.model_name:\n",
        "    for name, param in pretrained_model.named_parameters():\n",
        "        # freeze base model's layers\n",
        "        param.requires_grad = False\n",
        "\n",
        "        if getattr(pretrained_model, \"is_loaded_in_8bit\", False):\n",
        "            # cast layer norm in fp32 for stability for 8bit models\n",
        "            if param.ndim == 1 and \"layer_norm\" in name:\n",
        "                param.data = param.data.to(torch.float16)\n",
        "\n",
        "pretrained_model = prepare_model_for_int8_training(pretrained_model)\n",
        "pretrained_model = get_peft_model(pretrained_model, lora_config)\n",
        "\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(pretrained_model)\n",
        "\n",
        "model.gradient_checkpointing_disable = model.pretrained_model.gradient_checkpointing_disable\n",
        "model.gradient_checkpointing_enable = model.pretrained_model.gradient_checkpointing_enable\n",
        "\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "# GPT-2 tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.\n",
        "# only for this model.\n",
        "if \"llama\" in script_args.model_name:\n",
        "  tokenizer.pad_token = \" \"\n",
        "else:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.learning_rate)\n",
        "\n",
        "print(\"Initialize PPO framework\")\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config, model, ref_model=None, tokenizer=tokenizer, dataset=dataset['train'], data_collator=collator, optimizer=optimizer\n",
        ")\n",
        "\n",
        "\n",
        "device = ppo_trainer.accelerator.device\n",
        "if ppo_trainer.accelerator.num_processes == 1:\n",
        "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
        "\n",
        "print(\"Loading detector: ...\")\n",
        "detector_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base-openai-detector\")\n",
        "detectot_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base-openai-detector\")\n",
        "\n",
        "detector_model.load_state_dict(torch.load(script_args.detector_ckpt))\n",
        "\n",
        "#Text generation configs\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 0.96,\n",
        "    \"temperature\": 0.9,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "    \"eos_token_id\": -1,\n",
        "}\n",
        "\n",
        "\n",
        "break_epoch = 4\n",
        "gen_len = 64\n",
        "print(\"Training starting........\")\n",
        "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "\n",
        "    if epoch== break_epoch:\n",
        "      break\n",
        "\n",
        "    query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    model.gradient_checkpointing_disable()\n",
        "    model.pretrained_model.config.use_cache = True\n",
        "\n",
        "    # Get response from LLM - i.e. generations\n",
        "    response_tensors = []\n",
        "    for query in query_tensors:\n",
        "        # gen_len = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
        "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
        "        response_tensors.append(response.squeeze()[-gen_len:])\n",
        "\n",
        "    print(\"Response generation completed...\")\n",
        "    # batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "\n",
        "    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
        "\n",
        "    # Compute detector score - I added title + generation\n",
        "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "\n",
        "    prediction,y_test, logits = evaluate_openaidetect(model=detector_model, test_data=texts, tokenizer=detectot_tokenizer,\n",
        "                                                  test_labels=np.ones(len(texts)), max_sequence_length=256,\n",
        "                                                  test_batch_size=script_args.batch_size, device=device, return_logits=True)\n",
        "\n",
        "    # pipe_outputs = detector_pipe(texts, **sent_kwargs)\n",
        "    print(\"Reward generation completed...\")\n",
        "\n",
        "    # Reward is the class 0 - 'human'\n",
        "    rewards = [output for output in logits[0][0:, 1]]\n",
        "\n",
        "    print(\"Detector performance (F1): \", f1_score(y_test,prediction))\n",
        "\n",
        "    # Run PPO step\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model.pretrained_model.config.use_cache = False\n",
        "\n",
        "    print(\"PPO started...\")\n",
        "    print(texts[0])\n",
        "    # print(response_tensors)\n",
        "    print(rewards[0])\n",
        "\n",
        "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "    ppo_trainer.log_stats(stats, batch, rewards)\n",
        "\n",
        "peft_model_id = script_args.peft_model_id\n",
        "save_dir = \"\"\n",
        "\n",
        "if not os.path.exists(save_dir+peft_model_id):\n",
        "\n",
        "  os.makedirs(save_dir+peft_model_id)\n",
        "\n",
        "model.save_pretrained(save_dir+peft_model_id)"
      ],
      "metadata": {
        "id": "OXSyFMnWl_Xe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}